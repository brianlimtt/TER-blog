[← Home](https://brianlimtt.github.io/TER-blog/)<br>

# Key Points from *An Introduction to Statistical Learning (ISLP)*

## Table of Contents
1. [Statistical Learning Basics](#1-statistical-learning-basics)
2. [Linear Methods](#2-linear-methods)
3. [Resampling & Model Evaluation](#3-resampling--model-evaluation)
4. [Model Selection & Regularization](#4-model-selection--regularization)
5. [Nonlinear & Flexible Methods](#5-nonlinear--flexible-methods)
6. [Tree-Based Methods](#6-tree-based-methods)
7. [Support Vector Machines (SVMs)](#7-support-vector-machines-svms)
8. [Unsupervised Learning](#8-unsupervised-learning)
9. [Practical Insights](#9-practical-insights)

---

## 1. Statistical Learning Basics
- Methods for **predicting outcomes** and **understanding relationships** in data.
- Two main types:
  - **Supervised learning**: outcome variable known (e.g., regression, classification)
  - **Unsupervised learning**: outcome variable unknown (e.g., clustering, PCA)
- Important concepts: **bias–variance tradeoff**, **prediction vs inference**, **model interpretability**.

---

## 2. Linear Methods
- **Linear regression**: predict continuous outcomes.
- **Logistic regression**: predict binary outcomes.
- **Linear discriminant analysis (LDA)**: classify observations.

---

## 3. Resampling & Model Evaluation
- **Cross-validation**: estimate performance on unseen data.
- **Bootstrap**: estimate variability and uncertainty.

---

## 4. Model Selection & Regularization
- **Stepwise selection**: choose best subset of predictors.
- **Ridge regression & Lasso**: reduce overfitting.
- **Principal components regression (PCR)**: dimensionality reduction for better predictions.

---

## 5. Nonlinear & Flexible Methods
- **Polynomial regression, splines, GAMs**: model nonlinear relationships.
- Flexibility improves accuracy but may reduce interpretability.

---

## 6. Tree-Based Methods
- **Decision trees**: simple and interpretable.
- **Bagging & Random Forests**: reduce variance and improve accuracy.
- **Boosting**: sequentially improve weak learners.

---

## 7. Support Vector Machines (SVMs)
- Handle linear and nonlinear classification.
- **Kernels** allow projection into higher-dimensional spaces.

---

## 8. Unsupervised Learning
- **PCA (Principal Components Analysis)**: reduce dimensionality while retaining variance.
- **Clustering**: group similar observations (k-means, hierarchical).

---

## 9. Practical Insights
- Always check **model assumptions** before applying methods.
- Balance **accuracy vs interpretability**.
- Python labs in ISLP provide practical implementation examples.

---

*This summary is useful for students, researchers, or anyone looking to apply statistical learning in Python. The methods are also highly relevant for applications like quantitative trading, data science, and predictive analytics.*
