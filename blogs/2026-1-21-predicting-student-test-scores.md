[← Home](https://brianlimtt.github.io/TER-blog/)

# Predicting Student Test Scores: My Leaderboard Experience (Top 3%)

## Table of Contents
- [Introduction](#introduction)
- [My Results](#my-results)
- [What I Learned](#what-i-learned)
  - [1. Data Cleaning is Critical](#1-data-cleaning-is-critical)
  - [2. Exploratory Data Analysis is More Than Visuals](#2-exploratory-data-analysis-is-more-than-visuals)
  - [3. Feature Engineering Can Make or Break a Model](#3-feature-engineering-can-make-or-break-a-model)
  - [4. Blending Models Works Wonders](#4-blending-models-works-wonders)
  - [5. Metrics Matter](#5-metrics-matter)
- [Reflection](#reflection)
- [Next Steps](#next-steps)
- [Final Thoughts](#final-thoughts)

---

## Introduction

Over the past few weeks, I’ve been working on a project to **predict student test scores** using multiple datasets.  

The goal was to blend information from different sources, perform **exploratory data analysis (EDA)**, and create predictive models that could estimate student performance. Along the way, I learned valuable lessons about data cleaning, feature engineering, model blending, and interpreting results—lessons that go far beyond a simple leaderboard score.

---

## My Results

After carefully combining datasets, analyzing relationships, and experimenting with different modeling techniques, my **final leaderboard score** was:

### **Score: 8.54462**  
### **Rank: 87 / 2,851 participants**

Being in the **top 3%** was extremely rewarding. It validated the approach I took and showed the power of **systematic analysis and thoughtful modeling**.

---

## What I Learned

### 1. Data Cleaning is Critical
The raw datasets were messy, with:

- Inconsistent column names  
- Missing values  
- Slightly different formats across files  

Before building any model, I had to:

- Merge datasets on a **consistent primary key** (`name`)  
- Correctly handle missing or invalid values  
- Standardize numeric and categorical columns  

This reinforced a key lesson: **data preprocessing is often the most important part of any project**.

---

### 2. Exploratory Data Analysis is More Than Visuals
EDA is not just about making charts—it’s about understanding patterns and relationships in the data. I:

- Used **pairplots** to explore relationships between features and student scores  
- Calculated **correlation matrices** to identify strong predictors  
- Investigated **outliers** and inconsistencies that could skew results  

Through EDA, I learned which features were truly informative, which guided my modeling choices.

---

### 3. Feature Engineering Can Make or Break a Model
Raw data alone is rarely enough. I experimented with:

- Normalizing exam scores across different datasets  
- Combining multiple related features into new metrics  
- Encoding categorical features thoughtfully  

Even small changes in features often led to measurable improvements in predictive performance.

---

### 4. Blending Models Works Wonders
No single model was perfect. By **blending predictions** from multiple models, I reduced individual weaknesses and overfitting, which improved my leaderboard ranking.

---

### 5. Metrics Matter
The competition used **RMSE (Root Mean Squared Error)** as the evaluation metric. Small differences of **0.01 or 0.02** could move your rank by dozens of positions. I learned to:

- Monitor RMSE carefully during tuning  
- Focus on incremental improvements  
- Avoid transformations or rounding that could affect results  

Precision is key in competitive modeling.

---

## Reflection

Scoring **8.54462** and placing **87th out of 2,851** showed me that careful, step-by-step data science pays off. I gained practical experience in:

- Handling real-world, messy data  
- Understanding feature relationships  
- Making models that generalize well  

More importantly, I saw that **learning from the process** is more valuable than the score itself.

---

## Next Steps

Now that I’ve completed this project, I’m interested in exploring:

- More advanced modeling techniques such as ensemble methods and neural networks  
- Automated **feature selection** and hyperparameter optimization  
- Interpreting model predictions to extract deeper insights  

These steps will take my predictive skills to the next level.

---

## Final Thoughts

This project reinforced that success in data science requires:

- **Patience**  
- **Rigor**  
- **Attention to detail**  

While my score of **8.54462** is a concrete result, the lessons learned about **systematic analysis, blending models, and careful feature engineering** are far more valuable.  

Being in the top 3% was rewarding, but **understanding the “why” behind the score** is what will stick with me the most.
